import random
import numpy as np
import json
import os
import datetime
from typing import Dict, List, Any

# Ensure we can import from the same directory
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from .causal_brain_v6 import CausalBrainV6

class PatientGenerator:
    """
    Generates virtual patients based on sensitivity data from medical literature.
    Target Profile: True GAS Patient (Child)
    """
    def __init__(self):
        # Sensitivity Data (True Positive Rates)
        # Updated based on user feedback (V1.1)
        self.sensitivities = {
            'V_white': {'target': 'High', 'prob': 0.38, 'default': 'None'},
            'C_lymph': {'target': 'Anterior', 'prob': 0.40, 'default': 'Normal'},
            'V_vessel': {'target': 'Prominent', 'prob': 0.15, 'default': 'Normal'},
            'C_rash': {'target': 'Present', 'prob': 0.08, 'default': 'Absent'},
            'C_temp': {'target': 'High', 'prob': 0.53, 'default': 'Mild'}, 
            'C_cough': {'target': 'Absent', 'prob': 0.70, 'default': 'Present'},
            'C_pain_sev': {'target': 'Severe', 'prob': 0.90, 'default': 'Mild'},
        }
        self.fixed_context = {
            'Age_Group': 'Child', # Fixed as per spec
            # 'C_epidemic': 'None' # Not specified, assume None or random? Spec says "C_epidemic" not in list.
                                  # "V_white", "C_lymph", etc. are listed.
                                  # Let's assume standard background for others not listed.
        }

    def generate(self) -> Dict[str, Any]:
        patient = self.fixed_context.copy()
        
        for key, config in self.sensitivities.items():
            if random.random() < config['prob']:
                patient[key] = config['target']
            else:
                patient[key] = config['default']
        
        # Add random noise for other unlisted variables? 
        # Spec only lists these. CausalBrain requires some inputs, but can handle missing.
        # But wait, the model has many nodes. 
        # C_onset, C_duration etc.
        # If not provided, they are unknown. That's fine.
        
        return patient

def run_calibration(iterations: int = 1000, output_file: str = "calibration_result.json"):
    print(f"Starting Calibration ({iterations} iterations)...")
    
    # Load Model
    # Assumes running from root usually, but let's try to be robust with path
    # inference_model/model_config.yaml relative to this script
    current_dir = os.path.dirname(os.path.abspath(__file__))
    config_path = os.path.join(current_dir, "model_config.yaml")
    
    try:
        brain = CausalBrainV6(config_path)
    except Exception as e:
        print(f"Failed to load CausalBrainV6: {e}")
        return

    generator = PatientGenerator()
    conflict_scores = []
    gas_diagnosed_count = 0
    
    for i in range(iterations):
        # 1. Generate "True GAS" patient (Conceptually)
        ev = generator.generate()
        
        # 2. Run Inference
        result = brain.infer_cognitive(ev)
        
        # 3. Filter: Only count if Model actually diagnoses GAS
        # (If symptoms are too weak and it says Viral, it's not a "Conflict" in the sense of "High Confidence Wrongness" 
        # or "Silent Danger" usually, but spec says:
        # "Diagnosis result (diagnosis) became GAS cases only"
        
        if result.get('diagnosis') == 'GAS':
            gas_diagnosed_count += 1
            # 4. Collect Conflict Score
            if 'cognitive' in result:
                score = result['cognitive']['conflict_score']
                conflict_scores.append(score)
    
    if not conflict_scores:
        print("No cases were diagnosed as GAS. improving sensitivity might be needed or check thresholds.")
        return

    # 5. Statistics
    scores_np = np.array(conflict_scores)
    p90 = np.percentile(scores_np, 90)
    p95 = np.percentile(scores_np, 95)
    mean_score = np.mean(scores_np)
    
    print("-" * 30)
    print(f"Calibration Results (N={gas_diagnosed_count}/{iterations} Diagnosed GAS)")
    print(f"Mean Conflict: {mean_score:.4f}")
    print(f"P90 (Tau_Silent): {p90:.4f}")
    print(f"P95 (Tau_Over):   {p95:.4f}")
    print("-" * 30)
    
    # 6. Save results
    output_data = {
        "timestamp": datetime.datetime.now().isoformat(),
        "iterations": iterations,
        "diagnosed_gas_count": gas_diagnosed_count,
        "metrics": {
            "mean": float(mean_score),
            "p90": float(p90),
            "p95": float(p95)
        },
        "thresholds": {
            "tau_silent": float(p90),
            "tau_over": float(p95)
        },
        "note": "Generated by Conflict Threshold Calibration System V1.0"
    }
    
    # Save to json in same dir
    out_path = os.path.join(current_dir, output_file)
    with open(out_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=4, ensure_ascii=False)
    
    print(f"Results saved to {out_path}")

if __name__ == "__main__":
    run_calibration()
